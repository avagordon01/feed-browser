#!/usr/bin/env python3
import time
import feedparser
import concurrent.futures
import webbrowser
import appdirs
import pathlib
import argparse
import http
from browser_history import get_history

parser = argparse.ArgumentParser(
    prog='feed-browser',
)
parser.add_argument('--dry-run', type=bool, default=True, required=False)
parser.add_argument('--order', choices=['newest', 'oldest', 'random'], default='newest', required=False)
parser.add_argument('--read', type=bool, default=True, required=False)
parser.add_argument('--limit', type=int, default=10, required=False)
args = parser.parse_args()

config_dir = pathlib.Path(appdirs.user_config_dir('feed-browser'))
with open(config_dir / 'feeds.txt') as feeds_file:
    rss_urls = set([x.strip() for x in feeds_file.readlines()])

read_links = set(url for _, url, _ in get_history().histories)
print('got read_links')

def parse(url):
    try:
        return feedparser.parse(url)
    except http.client.RemoteDisconnected:
        print(f'remote disconnected without response {url}')

def entry_time(entry):
    if 'published_parsed' in entry:
        return entry['published_parsed']
    elif 'updated_parsed' in entry:
        return entry['updated_parsed']
    else:
        return time.gmtime(0)

with concurrent.futures.ThreadPoolExecutor(max_workers = 256) as executor:
    feeds = executor.map(parse, rss_urls)

entries = [item for feed in feeds if feed for item in feed['items']]
unread_links = sorted([x.link for x in entries if x.link not in read_links], key=entry_time, reverse=True)
unread_links = unread_links[0:args.limit]
print('got unread_links')

if not args.dry_run:
    [webbrowser.open(link) for link in unread_links]
else:
    print('\n'.join(unread_links))
